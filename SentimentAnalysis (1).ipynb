{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentimentAnalysis.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c44bc8f541aa4d639d876e524ace4b6e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c50db4925b454ce7bbd5446ea454cb90","IPY_MODEL_3862126a77c04c31956dc35acbfd2245","IPY_MODEL_8405081e7ae94e388079601b0a2150e2"],"layout":"IPY_MODEL_4e501d39bead4a4f99ae4ce5cfaeaaea"}},"c50db4925b454ce7bbd5446ea454cb90":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b03dcec09f89445bbe643291ba4212a1","placeholder":"​","style":"IPY_MODEL_4303ca7722ad4662a9b850c2f6df997e","value":"Downloading: 100%"}},"3862126a77c04c31956dc35acbfd2245":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb82e5bb7d9d4422a05ca99e25735095","max":536063208,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3d77561b6cf040c188f5c0f15bb3760b","value":536063208}},"8405081e7ae94e388079601b0a2150e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_85801716bc5b4d9d9d679a43c5d9ef00","placeholder":"​","style":"IPY_MODEL_6e83446fe7764c9cb44be2c964046a1d","value":" 511M/511M [00:18&lt;00:00, 50.6MB/s]"}},"4e501d39bead4a4f99ae4ce5cfaeaaea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b03dcec09f89445bbe643291ba4212a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4303ca7722ad4662a9b850c2f6df997e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb82e5bb7d9d4422a05ca99e25735095":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d77561b6cf040c188f5c0f15bb3760b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"85801716bc5b4d9d9d679a43c5d9ef00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e83446fe7764c9cb44be2c964046a1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# https://medium.com/@yashvardhanvs/classification-using-pre-trained-bert-model-transfer-learning-2d50f404ed4c"],"metadata":{"id":"zA7gSqqEalT-"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Aj_BinWMR3Ri"},"outputs":[],"source":["!pip install transformers\n","\n","!pip install wget\n","import wget\n","\n","import pandas as pd\n","import numpy as np\n","import regex as re\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)\n","\n","from tensorflow.keras.utils import to_categorical\n","from keras.preprocessing.text import Tokenizer\n","\n","import transformers\n","import keras\n","\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping\n","from tensorflow.keras.initializers import TruncatedNormal\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","from tensorflow.keras.metrics import CategoricalAccuracy\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.layers import Input, Dense, Dropout, GlobalMaxPool1D, Embedding\n","\n","from tensorflow.keras.models import Sequential\n","\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","\n","import string\n","\n","from transformers import BertTokenizer, TFBertModel, BertConfig, TFBertForSequenceClassification"],"metadata":{"id":"yAQc0MWqUeQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Loading and Cleaning DataSet"],"metadata":{"id":"nFeivxzaTu6o"}},{"cell_type":"code","source":["df = pd.read_csv ('gdrive/MyDrive/Colab Notebooks/Sentiment Analysis/tweet_emotions.csv') # loading the dataset\n","\n","df.drop('tweet_id', inplace = True, axis = 1) # removing tweet_id column\n","df.drop(df[(df['sentiment'] == 'empty')].index, inplace=True) # removing empty sentiment rows\n","df.rename(columns = {'content':'sentence'}, inplace = True) # renaming 'content' column to 'sentence'\n","\n","df.replace(to_replace = ['fun', 'happiness'], value = \"party\", inplace = True) # fun, happiness <-> party\n","df.replace(to_replace = ['enthusiasm', 'surprise', 'love'], value = \"happy\", inplace = True) # enthusiasm, surprise, love <-> happy\n","df.replace(to_replace = ['sadness', 'anger', 'hate'], value = \"sad\", inplace = True) # sadness, anger <-> sad\n","df.replace(to_replace = ['boredom', 'worry'], value = \"chill\", inplace = True) # boredom, worry <-> chill\n","df.replace(to_replace = ['neutral', 'relief'], value = \"normal\", inplace = True) # neutral, relief <-> normal\n","\n","df.dropna(inplace=True) # removing empty values"],"metadata":{"id":"1NgXNjR9SYM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def clean_data(df):\n","  df['sentence'] = df['sentence'].apply(lambda x: re.sub('http\\S+', \"\", x))\n","  df['sentence'] = df['sentence'].apply(lambda x: re.sub(r\"\\@\\w+\", \"\", x))\n","  df['sentence'] = df['sentence'].apply(lambda x: re.sub(r\"\\#\\w+\", \"\", x))\n","  df['sentence'] = df['sentence'].apply(lambda x: re.sub(r'[^\\w]', \" \", x))\n","  df['sentence'] = df['sentence'].apply(lambda x: re.sub(r\"\\............\\w+\", \"\", x))\n","  df['sentence'] = df['sentence'].apply(lambda x: \" \".join(x.split()))\n","\n","  sentiments_dict = {'party':0, 'happy':1, 'sad':2, 'chill':3, 'normal':4} # making sentiment dictionary\n","\n","  for i in range(1, 40000):\n","    try:\n","      df['sentiment'][i] = sentiments_dict[df['sentiment'][i]] \n","    except:\n","      continue\n","\n","  return df"],"metadata":{"id":"AmDoutKGVZId"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cleaned_data = clean_data(df)"],"metadata":{"id":"zXc4oHBxViuP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Splitting Data"],"metadata":{"id":"HSBha0U1YbEo"}},{"cell_type":"code","source":["tweets = cleaned_data.sentence.values # making an array of tweets\n","labels = cleaned_data.sentiment.values # making an array of sentiments\\"],"metadata":{"id":"nMbi0azfYd6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input, val_input, train_label, val_label = train_test_split(tweets, labels)"],"metadata":{"id":"sUYZDzVSaLqx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Shape of train input and label should be the same\")\n","print(train_input.shape)\n","print(train_label.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXby-9WFNZh5","executionInfo":{"status":"ok","timestamp":1658135957163,"user_tz":-330,"elapsed":8,"user":{"displayName":"Pratik Hazarika","userId":"04317859684722913646"}},"outputId":"92dee5e4-6057-4b90-9ec9-7f77ce87f94e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of train input and label should be the same\n","(29379,)\n","(29379,)\n"]}]},{"cell_type":"markdown","source":["# Tokenizing"],"metadata":{"id":"bp8kIm-HQIBd"}},{"cell_type":"code","source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"],"metadata":{"id":"byCRMhkCni-V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_len = 0\n","\n","for tweet in tweets:\n","  max_len = max(max_len, len(tweet))\n","\n","print('Max len', max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yacNBw0hQojE","executionInfo":{"status":"ok","timestamp":1658135977412,"user_tz":-330,"elapsed":4,"user":{"displayName":"Pratik Hazarika","userId":"04317859684722913646"}},"outputId":"65a1c826-c66d-4833-e58e-a355dbd29ce3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Max len 158\n"]}]},{"cell_type":"markdown","source":["# Embedding"],"metadata":{"id":"Fa9oKJzinDwm"}},{"cell_type":"code","source":["def mask_inputs_for_bert(tweets, max_len):\n","  input_ids = []\n","  attention_mask = []\n","\n","  for tweet in tweets:\n","    encoded_dict = tokenizer.encode_plus(tweet, add_special_tokens = True, max_length = max_len, pad_to_max_length = True, return_attention_mask = True)\n","    input_ids.append(encoded_dict['input_ids'])\n","    attention_mask.append(encoded_dict['attention_mask'])\n","\n","  input_ids = tf.convert_to_tensor(input_ids)\n","  attention_mask = tf.convert_to_tensor(attention_mask)\n","\n","  return input_ids, attention_mask"],"metadata":{"id":"--bUfVHKR-tY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_input, train_mask = mask_inputs_for_bert(train_input, max_len)\n","val_input, val_mask = mask_inputs_for_bert(val_input, max_len)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNFiOHZ8Tfkw","executionInfo":{"status":"ok","timestamp":1658136007553,"user_tz":-330,"elapsed":25990,"user":{"displayName":"Pratik Hazarika","userId":"04317859684722913646"}},"outputId":"48305c7a-a304-473a-d80e-b2e40f6a0bf1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","source":["print(f\"train input: {train_input.shape}\")\n","print(f\"mask input: {train_mask.shape}\")\n","print()\n","print(f\"val input: {val_input.shape}\")\n","print(f\"val mask: {val_mask.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhlXQyjshf9Z","executionInfo":{"status":"ok","timestamp":1658136028063,"user_tz":-330,"elapsed":606,"user":{"displayName":"Pratik Hazarika","userId":"04317859684722913646"}},"outputId":"0f46d5fb-21e3-432d-8357-1ed517e79c0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train input: (29379, 158)\n","mask input: (29379, 158)\n","\n","val input: (9794, 158)\n","val mask: (9794, 158)\n"]}]},{"cell_type":"code","source":["train_label = np.asarray(train_label).astype(np.int32)\n","train_label = tf.convert_to_tensor(train_label)\n","\n","val_label = np.asarray(val_label).astype(np.int32)\n","val_label = tf.convert_to_tensor(val_label)"],"metadata":{"id":"0jL5K5jchM2H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"train label: {train_label.shape}\")\n","print(f\"val label: {val_label.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_NAjomKTnlI","executionInfo":{"status":"ok","timestamp":1658136033082,"user_tz":-330,"elapsed":518,"user":{"displayName":"Pratik Hazarika","userId":"04317859684722913646"}},"outputId":"405e5bb7-185d-4c3c-f421-ed85d0893f09"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train label: (29379,)\n","val label: (9794,)\n"]}]},{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"p--GaxLhnIU5"}},{"cell_type":"code","source":["from transformers import TFBertForSequenceClassification\n","bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["c44bc8f541aa4d639d876e524ace4b6e","c50db4925b454ce7bbd5446ea454cb90","3862126a77c04c31956dc35acbfd2245","8405081e7ae94e388079601b0a2150e2","4e501d39bead4a4f99ae4ce5cfaeaaea","b03dcec09f89445bbe643291ba4212a1","4303ca7722ad4662a9b850c2f6df997e","bb82e5bb7d9d4422a05ca99e25735095","3d77561b6cf040c188f5c0f15bb3760b","85801716bc5b4d9d9d679a43c5d9ef00","6e83446fe7764c9cb44be2c964046a1d"]},"id":"ta7e6qhjkN0Z","executionInfo":{"status":"ok","timestamp":1658136182996,"user_tz":-330,"elapsed":25401,"user":{"displayName":"Pratik Hazarika","userId":"04317859684722913646"}},"outputId":"c8ae1604-bfbe-46af-d01b-be83a9297be1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/511M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c44bc8f541aa4d639d876e524ace4b6e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n","\n","Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["log_dir = 'gdrive/MyDrive/Colab Notebooks/Sentiment Analysis'\n","model_save_path = 'gdrive/MyDrive/Colab Notebooks/Sentiment Analysis/bert_model.h5'\n","\n","callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,\n","                                                save_weights_only=True,\n","                                                monitor='val_loss',\n","                                                mode='min',\n","                                                save_best_only=True),\n","                                                keras.callbacks.TensorBoard(log_dir=log_dir)]"],"metadata":{"id":"7wxaq3Mjndfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","metric = tf.keras.metrics.SparseCategoricalAccuracy ('accuracy')\n","optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)"],"metadata":{"id":"rbg9_WAKpfvc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bert_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])"],"metadata":{"id":"YwDrc5Tlq0tF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = bert_model.fit([train_input, train_mask],\n","                         train_label,\n","                         batch_size = 128,\n","                         epochs = 4,\n","                         validation_data = ([val_input, val_mask], val_label),\n","                         callbacks=callbacks)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MCpke82od8F","outputId":"992a43f7-84f8-4e98-cb05-d7f9074917a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"1M3RjUtBpMjO"},"execution_count":null,"outputs":[]}]}